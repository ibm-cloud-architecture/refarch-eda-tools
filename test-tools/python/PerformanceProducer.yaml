apiVersion: batch/v1
kind: Job
metadata:
  name: performance-test-producer
spec:
  # Controls how many times this job must be run
  completions: 1
  # Controls how many jobs can run in parallel (the higher the more input throughput)
  parallelism: 1
  template:
    metadata:
      name: performance-producer
    spec:
      containers:
      - name: python-es-performance
        image: ibmcase/python-es-performance:latest
        imagePullPolicy: Always
        command:
        - /bin/bash
        - -c
        - |
              echo "-------------------------------"
              echo "-- Performance test producer --"
              echo "-------------------------------"
              echo
              echo "Executing the python performance producer"
              echo "-----------------------------------------"
              echo "Kafka Brokers: $KAFKA_BROKERS"
              echo "Kafka API Key: $KAFKA_APIKEY"
              echo "Kafka ES certificate: $PEM_CERT"
              echo "Topic name: $TOPIC_NAME"
              echo "-----------------------------------------"
              echo

              cd ${WORKING_DIR}

              # Check important environments variables are properly set
              
              # Check the ES Kafka brokers is set
              if [ -z "$KAFKA_BROKERS" ]; then
                echo "[ERROR] - KAFKA_BROKERS environment variable must be set."
                exit 1
              fi
              
              # Check the Event Streams API key is set
              if [ -z "$KAFKA_APIKEY" ]; then
                echo "[ERROR] - KAFKA_APIKEY environment variable must be set."
                exit 1
              fi

              # Check that if we are using IBM Event Streams on prem, we are providing the PEM certificate to connect to it.
              if [ -f "$PEM_CERT" ]; then
                export KAFKA_CERT=${PEM_CERT}
              fi

              # Check that if we are using a provided data file
              if [ -f "$DATA_FILE" ]; then
                export FILE=${DATA_FILE}
              fi

              # Start the producer
              
              export PYTHONPATH=${PYTHONPATH}:${WORKING_DIR}

              python -u ProducerPerformance.py --size ${SIZE} --file ${FILE} --topic ${TOPIC_NAME} --keyname ${KEYNAME}

              echo "---------"
              echo "-- END --"
              echo "---------"
        env:
        # Your working directory within the container
        - name: WORKING_DIR
          value: "/tmp"
        # Topic name where the producer will send messages to
        - name: TOPIC_NAME
          value: "performance-test"
        # Number of messages to be sent
        - name: SIZE
          value: "10"
        # File where the message(s) to be sent are
        - name: FILE
          value: "./data/testplayload.json"
        # Attribute name to be used as the key of the message(s) sent
        - name: KEYNAME
          value: "identifier"
        # Event Streams Kafka brokers. You MUST create a configmap in advance with the brokers information:
        # oc create configmap kafka-brokers --from-literal=brokers='<replace with comma-separated list of brokers>' -n <namespace>
        # You can find the list of brokers either in the Event Streams UI or when you login through the CLI: cloudctl es init
        - name: KAFKA_BROKERS
          valueFrom:
            configMapKeyRef:
              name: "kafka-brokers"
              key: brokers
        # Your Event Streams API Key. You can find it in the Event Streams UI on the connect to this cluster link
        # You MUST create this secret in advance with the Event Streams API key:
        # oc create secret generic eventstreams-apikey --from-literal=binding='<replace with api key>' -n <namespace>
        - name: KAFKA_APIKEY
          valueFrom:
            secretKeyRef:
              name: "eventstreams-apikey"
              key: binding
              optional: true
        # Location of your Event Streams pem certificate for ssl connections. You MUST download it in advance.
        # This can be downloaded from the Event Streams UI.
        - name: PEM_CERT
          value: "/tmp/certs/es-cert.pem"
        # Location of your data file to be used to send message(s).
        - name: DATA_FILE
          value: "/tmp/performance_data/data.json"
        # Location where the Event Streams pem certificate contained in the eventstreams-pem-file secret will be loaded to.
        volumeMounts:
        - mountPath: "/tmp/certs"
          name: eventstreams-pem-file
        # Location where the file with the data to be sent as message(s) is
        - mountPath: "/tmp/performance_data"
          name: data-file
      volumes:
         # You MUST create this secret with the Event Streams pem certificate in advance. First, download the Event Streams pem certificate.
         # Then create the secret: oc create secret generic eventstreams-pem-file --from-file=es-cert.pem=<LOCATION_OF_YOUR_EVENT_STREAMS_PEM_CERTIFICATE> -n <namespace>
         - name: eventstreams-pem-file
           secret:
             secretName: "eventstreams-pem-file"
             optional: true
         # You MUST create this configMap with the data you want to send in advance. The file containing the data MUST be called data.json
         # Create the configMap: oc create configmap data-file --from-file <LOCATION_OF_YOUR_DATA_FILE> -n <namespace>
         - name: data-file
           configMap:
             name: "data-file"
             optional: true
      restartPolicy: Never
  backoffLimit: 0
